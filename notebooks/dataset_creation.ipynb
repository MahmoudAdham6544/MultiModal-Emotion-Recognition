{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0374c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, data_array, face_root, label_map, transform=None, sequence_len=5):\n",
    "        \"\"\"\n",
    "        data_array: [(mfcc, folder_name, label), ...]\n",
    "        face_root: path to the folder containing subfolders with face images\n",
    "        label_map: {'happy': 0, ...}\n",
    "        transform: torchvision transform\n",
    "        sequence_len: number of face frames to load per sample\n",
    "        \"\"\"\n",
    "        self.data = data_array\n",
    "        self.face_root = face_root\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc, folder_name, label = self.data[idx]\n",
    "\n",
    "        # Load all face image paths from the folder\n",
    "        face_folder = os.path.join(self.face_root, folder_name)\n",
    "        face_files = [\n",
    "        os.path.join(face_folder, fname)\n",
    "        for fname in os.listdir(face_folder)\n",
    "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "        face_files.sort()  # Sort to ensure consistent ordering\n",
    "        if len(face_files) < self.sequence_len:\n",
    "            raise ValueError(f\"Not enough face images in {face_folder} (found {len(face_files)}, expected {self.sequence_len})\")\n",
    "\n",
    "        # Select evenly spaced frames across the folder\n",
    "        step = len(face_files) // self.sequence_len\n",
    "        selected_files = face_files[::step][:self.sequence_len]\n",
    "\n",
    "        face_sequence = []\n",
    "        for file in selected_files:\n",
    "            img = Image.open(file).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            face_sequence.append(img)\n",
    "\n",
    "        # Stack into shape: (sequence_len, 3, H, W)\n",
    "        face_tensor = torch.stack(face_sequence)\n",
    "\n",
    "        # Convert MFCC to (1, 40, T)\n",
    "        mfcc_tensor = torch.tensor(mfcc.T, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label_tensor = torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "        return face_tensor, mfcc_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5e8e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"../data/Processed/final_dataset.npy\"\n",
    "data_array = np.load(dataset_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a785d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "#emotions (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "label_map = {'neutral': 1, 'calm': 2, 'happy': 3, 'sad':4, 'angry':5, 'fearful': 6, 'disgust':7, 'surprised':8}  # Example label map\n",
    "\n",
    "dataset = MultimodalEmotionDataset(\n",
    "    data_array,\n",
    "    face_root=\"../data/Processed/faces/\",\n",
    "    label_map=label_map,\n",
    "    transform=transform,\n",
    "    sequence_len=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45a65c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Train + (Val + Test)\n",
    "train_data, valtest_data = train_test_split(data_array, test_size=0.3, random_state=42, stratify=[label for _, _, label in data_array])\n",
    "\n",
    "# Second split: Val + Test\n",
    "val_data, test_data = train_test_split(valtest_data, test_size=0.5, random_state=42, stratify=[label for _, _, label in valtest_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5b80823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultimodalEmotionDataset(train_data, face_root=\"data/faces\", label_map=label_map, transform=transform)\n",
    "val_dataset = MultimodalEmotionDataset(val_data, face_root=\"data/faces\", label_map=label_map, transform=transform)\n",
    "test_dataset = MultimodalEmotionDataset(test_data, face_root=\"data/faces\", label_map=label_map, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "721ac93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bd8d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_split.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03b61d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo_recog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
