{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, data_array, face_root, label_map, transform=None, sequence_len=5):\n",
    "        \"\"\"\n",
    "        data_array: [(mfcc, folder_name, label), ...]\n",
    "        face_root: path to the folder containing subfolders with face images\n",
    "        label_map: {'happy': 0, ...}\n",
    "        transform: torchvision transform\n",
    "        sequence_len: number of face frames to load per sample\n",
    "        \"\"\"\n",
    "        self.data = data_array\n",
    "        self.face_root = face_root\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mfcc, folder_name, label = self.data[idx]\n",
    "\n",
    "        # Load all face image paths from the folder\n",
    "        face_folder = os.path.join(self.face_root, folder_name)\n",
    "        face_files = [\n",
    "        os.path.join(face_folder, fname)\n",
    "        for fname in os.listdir(face_folder)\n",
    "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "        ]\n",
    "        face_files.sort()  # Sort to ensure consistent ordering\n",
    "        if len(face_files) < self.sequence_len:\n",
    "            raise ValueError(f\"Not enough face images in {face_folder} (found {len(face_files)}, expected {self.sequence_len})\")\n",
    "\n",
    "        # Select evenly spaced frames across the folder\n",
    "        step = len(face_files) // self.sequence_len\n",
    "        selected_files = face_files[::step][:self.sequence_len]\n",
    "\n",
    "        face_sequence = []\n",
    "        for file in selected_files:\n",
    "            img = Image.open(file).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            face_sequence.append(img)\n",
    "\n",
    "        # Stack into shape: (sequence_len, 3, H, W)\n",
    "        face_tensor = torch.stack(face_sequence)\n",
    "\n",
    "        # Convert MFCC to (1, 40, T)\n",
    "        mfcc_tensor = torch.tensor(mfcc.T, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label_tensor = torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "        return face_tensor, mfcc_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e8e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"../data/Processed/final_dataset.npy\"\n",
    "data_array = np.load(dataset_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a785d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "#emotions (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "label_map = {'neutral': 1, 'calm': 2, 'happy': 3, 'sad':4, 'angry':5, 'fearful': 6, 'disgust':7, 'surprised':8}  # Example label map\n",
    "\n",
    "dataset = MultimodalEmotionDataset(\n",
    "    data_array,\n",
    "    face_root=\"../data/Processed/faces/\",\n",
    "    label_map=label_map,\n",
    "    transform=transform,\n",
    "    sequence_len=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a65c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: Train + (Val + Test)\n",
    "train_data, valtest_data = train_test_split(data_array, test_size=0.3, random_state=42, stratify=[label for _, _, label in data_array])\n",
    "\n",
    "# Second split: Val + Test\n",
    "val_data, test_data = train_test_split(valtest_data, test_size=0.5, random_state=42, stratify=[label for _, _, label in valtest_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b80823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultimodalEmotionDataset(train_data, face_root=\"data/faces\", label_map=label_map, transform=transform)\n",
    "val_dataset = MultimodalEmotionDataset(val_data, face_root=\"data/faces\", label_map=label_map, transform=transform)\n",
    "test_dataset = MultimodalEmotionDataset(test_data, face_root=\"data/faces\", label_map=label_map, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "721ac93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca03b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class VisualBackbone(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # output: (B, 512, 1, 1)\n",
    "        self.fc = nn.Linear(512, embed_dim)\n",
    "\n",
    "    def forward(self, x_seq):  # x_seq: (B, T, 3, H, W)\n",
    "        B, T, C, H, W = x_seq.shape\n",
    "        x_seq = x_seq.view(B * T, C, H, W)\n",
    "        features = self.cnn(x_seq)       # (B*T, 512, 1, 1)\n",
    "        features = features.view(B, T, -1).mean(dim=1)  # average over time\n",
    "        return self.fc(features)         # (B, embed_dim)\n",
    "\n",
    "class AudioBranch(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=40, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(128 * 2, embed_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, 1, 40, T)\n",
    "        x = x.squeeze(1).permute(0, 2, 1)  # â†’ (B, T, 40)\n",
    "        _, (hn, _) = self.lstm(x)         # hn shape: (4, B, 128)\n",
    "        out = torch.cat((hn[-2], hn[-1]), dim=1)  # (B, 256)\n",
    "        return self.fc(out)               # (B, embed_dim)\n",
    "\n",
    "class MultimodalEmotionRecognizer(nn.Module):\n",
    "    def __init__(self, num_classes=8, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.visual_branch = VisualBackbone(embed_dim)\n",
    "        self.audio_branch = AudioBranch(embed_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, face_seq, mfcc):\n",
    "        visual_feat = self.visual_branch(face_seq)  # (B, embed_dim)\n",
    "        audio_feat  = self.audio_branch(mfcc)       # (B, embed_dim)\n",
    "        fused = torch.cat((visual_feat, audio_feat), dim=1)\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80f61cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "face_dir = \"../data/Processed/faces/\"\n",
    "label_map = {'neutral': 1, 'calm': 2, 'happy': 3, 'sad':4, 'angry':5, 'fearful': 6, 'disgust':7, 'surprised':8}  # Example label map\n",
    "batch_size = 32\n",
    "num_classes = len(label_map)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for faces, mfccs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        faces, mfccs, labels = faces.to(device), mfccs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(faces, mfccs)           # [B, num_classes]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # stats\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct   += (preds == labels).sum().item()\n",
    "        total     += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for faces, mfccs, labels in tqdm(loader, desc=\"Valid\", leave=False):\n",
    "            faces, mfccs, labels = faces.to(device), mfccs.to(device), labels.to(device)\n",
    "            outputs = model(faces, mfccs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct   += (preds == labels).sum().item()\n",
    "            total     += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def main():\n",
    "    # 1. Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # 2. Datasets & Loaders\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    train_ds = MultimodalEmotionDataset(\n",
    "        train_data, face_root=face_dir,\n",
    "        label_map=label_map, transform=transform,\n",
    "        sequence_len=5)\n",
    "    val_ds = MultimodalEmotionDataset(\n",
    "        val_dataset, face_root=face_dir,\n",
    "        label_map=label_map, transform=transform,\n",
    "        sequence_len=5)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 3. Model, Loss, Optimizer\n",
    "    model = MultimodalEmotionRecognizer(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    # 4. Epoch loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss,   val_acc   = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"  Train â€” loss: {train_loss:.4f}, acc: {train_acc:.4f}\")\n",
    "        print(f\"  Valid â€” loss: {val_loss:.4f}, acc: {val_acc:.4f}\")\n",
    "\n",
    "        # 5. Checkpoint\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            ckpt_path = f\"checkpoints/best_epoch{epoch:02d}_acc{val_acc:.3f}.pth\"\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"  Saved best model to {ckpt_path}\")\n",
    "\n",
    "    print(\"\\nTraining complete. Best valid acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24786c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo_recog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
